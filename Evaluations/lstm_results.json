{"Feminist Movement": {"stance_or_none": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 126}, "1": {"precision": 0.8102409638554217, "recall": 1.0, "f1-score": 0.8951747088186357, "support": 538}, "accuracy": 0.8102409638554217, "macro avg": {"precision": 0.40512048192771083, "recall": 0.5, "f1-score": 0.44758735440931785, "support": 664}, "weighted avg": {"precision": 0.6564904195093627, "recall": 0.8102409638554217, "f1-score": 0.7253072188922078, "support": 664}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 44}, "1": {"precision": 0.8456140350877193, "recall": 1.0, "f1-score": 0.9163498098859315, "support": 241}, "accuracy": 0.8456140350877193, "macro avg": {"precision": 0.42280701754385963, "recall": 0.5, "f1-score": 0.45817490494296575, "support": 285}, "weighted avg": {"precision": 0.7150630963373346, "recall": 0.8456140350877193, "f1-score": 0.774878260289507, "support": 285}}}, "favor_or_against": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 210}, "1": {"precision": 0.6096654275092936, "recall": 1.0, "f1-score": 0.7575057736720554, "support": 328}, "accuracy": 0.6096654275092936, "macro avg": {"precision": 0.3048327137546468, "recall": 0.5, "f1-score": 0.3787528868360277, "support": 538}, "weighted avg": {"precision": 0.3716919335000898, "recall": 0.6096654275092936, "f1-score": 0.4618250813465319, "support": 538}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 58}, "1": {"precision": 0.7593360995850622, "recall": 1.0, "f1-score": 0.8632075471698113, "support": 183}, "accuracy": 0.7593360995850622, "macro avg": {"precision": 0.3796680497925311, "recall": 0.5, "f1-score": 0.43160377358490565, "support": 241}, "weighted avg": {"precision": 0.5765913121330555, "recall": 0.7593360995850622, "f1-score": 0.6554646520003131, "support": 241}}}}, "Climate Change is a Real Concern": {"stance_or_none": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 168}, "1": {"precision": 0.5746835443037974, "recall": 1.0, "f1-score": 0.7299035369774919, "support": 227}, "accuracy": 0.5746835443037974, "macro avg": {"precision": 0.2873417721518987, "recall": 0.5, "f1-score": 0.36495176848874594, "support": 395}, "weighted avg": {"precision": 0.3302611760935747, "recall": 0.5746835443037974, "f1-score": 0.4194635516301029, "support": 395}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 35}, "1": {"precision": 0.7928994082840237, "recall": 1.0, "f1-score": 0.8844884488448845, "support": 134}, "accuracy": 0.7928994082840237, "macro avg": {"precision": 0.39644970414201186, "recall": 0.5, "f1-score": 0.44224422442244227, "support": 169}, "weighted avg": {"precision": 0.6286894716571549, "recall": 0.7928994082840237, "f1-score": 0.7013103677231629, "support": 169}}}, "favor_or_against": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 212}, "1": {"precision": 0.06607929515418502, "recall": 1.0, "f1-score": 0.12396694214876032, "support": 15}, "accuracy": 0.06607929515418502, "macro avg": {"precision": 0.03303964757709251, "recall": 0.5, "f1-score": 0.06198347107438016, "support": 227}, "weighted avg": {"precision": 0.0043664732480739, "recall": 0.06607929515418502, "f1-score": 0.008191648159609712, "support": 227}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 123}, "1": {"precision": 0.08208955223880597, "recall": 1.0, "f1-score": 0.1517241379310345, "support": 11}, "accuracy": 0.08208955223880597, "macro avg": {"precision": 0.041044776119402986, "recall": 0.5, "f1-score": 0.07586206896551725, "support": 134}, "weighted avg": {"precision": 0.006738694586767654, "recall": 0.08208955223880597, "f1-score": 0.01245496654657746, "support": 134}}}}, "Hillary Clinton": {"stance_or_none": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 166}, "1": {"precision": 0.7402190923317684, "recall": 1.0, "f1-score": 0.8507194244604317, "support": 473}, "accuracy": 0.7402190923317684, "macro avg": {"precision": 0.3701095461658842, "recall": 0.5, "f1-score": 0.42535971223021585, "support": 639}, "weighted avg": {"precision": 0.5479243046524671, "recall": 0.7402190923317684, "f1-score": 0.6297187602031051, "support": 639}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 78}, "1": {"precision": 0.735593220338983, "recall": 1.0, "f1-score": 0.84765625, "support": 217}, "accuracy": 0.735593220338983, "macro avg": {"precision": 0.3677966101694915, "recall": 0.5, "f1-score": 0.423828125, "support": 295}, "weighted avg": {"precision": 0.5410973858086757, "recall": 0.735593220338983, "f1-score": 0.6235301906779661, "support": 295}}}, "favor_or_against": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 112}, "1": {"precision": 0.7632135306553911, "recall": 1.0, "f1-score": 0.8657074340527577, "support": 361}, "accuracy": 0.7632135306553911, "macro avg": {"precision": 0.38160676532769555, "recall": 0.5, "f1-score": 0.43285371702637887, "support": 473}, "weighted avg": {"precision": 0.5824948933754677, "recall": 0.7632135306553911, "f1-score": 0.6607196272580244, "support": 473}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 45}, "1": {"precision": 0.7926267281105991, "recall": 1.0, "f1-score": 0.884318766066838, "support": 172}, "accuracy": 0.7926267281105991, "macro avg": {"precision": 0.39631336405529954, "recall": 0.5, "f1-score": 0.442159383033419, "support": 217}, "weighted avg": {"precision": 0.6282571301153135, "recall": 0.7926267281105991, "f1-score": 0.7009346901543602, "support": 217}}}}, "Legalization of Abortion": {"stance_or_none": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 164}, "1": {"precision": 0.7280265339966833, "recall": 1.0, "f1-score": 0.8426103646833014, "support": 439}, "accuracy": 0.7280265339966833, "macro avg": {"precision": 0.36401326699834163, "recall": 0.5, "f1-score": 0.4213051823416507, "support": 603}, "weighted avg": {"precision": 0.5300226342032238, "recall": 0.7280265339966833, "f1-score": 0.6134427033100652, "support": 603}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 45}, "1": {"precision": 0.8392857142857143, "recall": 1.0, "f1-score": 0.9126213592233009, "support": 235}, "accuracy": 0.8392857142857143, "macro avg": {"precision": 0.41964285714285715, "recall": 0.5, "f1-score": 0.45631067961165045, "support": 280}, "weighted avg": {"precision": 0.7044005102040817, "recall": 0.8392857142857143, "f1-score": 0.7659500693481275, "support": 280}}}, "favor_or_against": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 105}, "1": {"precision": 0.7608200455580866, "recall": 1.0, "f1-score": 0.8641655886157827, "support": 334}, "accuracy": 0.7608200455580866, "macro avg": {"precision": 0.3804100227790433, "recall": 0.5, "f1-score": 0.43208279430789137, "support": 439}, "weighted avg": {"precision": 0.5788471417230089, "recall": 0.7608200455580866, "f1-score": 0.6574745025003905, "support": 439}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 46}, "1": {"precision": 0.8042553191489362, "recall": 1.0, "f1-score": 0.8915094339622642, "support": 189}, "accuracy": 0.8042553191489362, "macro avg": {"precision": 0.4021276595744681, "recall": 0.5, "f1-score": 0.4457547169811321, "support": 235}, "weighted avg": {"precision": 0.6468266183793572, "recall": 0.8042553191489362, "f1-score": 0.7170012043356083, "support": 235}}}}, "Atheism": {"stance_or_none": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 117}, "1": {"precision": 0.7719298245614035, "recall": 1.0, "f1-score": 0.8712871287128713, "support": 396}, "accuracy": 0.7719298245614035, "macro avg": {"precision": 0.38596491228070173, "recall": 0.5, "f1-score": 0.43564356435643564, "support": 513}, "weighted avg": {"precision": 0.5958756540473992, "recall": 0.7719298245614035, "f1-score": 0.6725725204099358, "support": 513}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 28}, "1": {"precision": 0.8727272727272727, "recall": 1.0, "f1-score": 0.9320388349514563, "support": 192}, "accuracy": 0.8727272727272727, "macro avg": {"precision": 0.43636363636363634, "recall": 0.5, "f1-score": 0.46601941747572817, "support": 220}, "weighted avg": {"precision": 0.7616528925619834, "recall": 0.8727272727272727, "f1-score": 0.8134157105030891, "support": 220}}}, "favor_or_against": {"train": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 92}, "1": {"precision": 0.7676767676767676, "recall": 1.0, "f1-score": 0.8685714285714284, "support": 304}, "accuracy": 0.7676767676767676, "macro avg": {"precision": 0.3838383838383838, "recall": 0.5, "f1-score": 0.4342857142857142, "support": 396}, "weighted avg": {"precision": 0.5893276196306498, "recall": 0.7676767676767676, "f1-score": 0.6667821067821067, "support": 396}}, "test": {"0": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 32}, "1": {"precision": 0.8333333333333334, "recall": 1.0, "f1-score": 0.9090909090909091, "support": 160}, "accuracy": 0.8333333333333334, "macro avg": {"precision": 0.4166666666666667, "recall": 0.5, "f1-score": 0.45454545454545453, "support": 192}, "weighted avg": {"precision": 0.6944444444444445, "recall": 0.8333333333333334, "f1-score": 0.7575757575757575, "support": 192}}}}}