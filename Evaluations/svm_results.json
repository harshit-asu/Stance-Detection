{"Hillary Clinton": {"stance_or_none": {"train": {"0": {"precision": 0.9473684210526315, "recall": 0.3253012048192771, "f1-score": 0.48430493273542596, "support": 166}, "1": {"precision": 0.8075601374570447, "recall": 0.9936575052854123, "f1-score": 0.8909952606635072, "support": 473}, "accuracy": 0.8200312989045383, "macro avg": {"precision": 0.8774642792548382, "recall": 0.6594793550523447, "f1-score": 0.6876500966994665, "support": 639}, "weighted avg": {"precision": 0.8438796602690437, "recall": 0.8200312989045383, "f1-score": 0.7853448781344595, "support": 639}}, "test": {"0": {"precision": 0.8181818181818182, "recall": 0.11538461538461539, "f1-score": 0.20224719101123595, "support": 78}, "1": {"precision": 0.7570422535211268, "recall": 0.9907834101382489, "f1-score": 0.8582834331337325, "support": 217}, "accuracy": 0.7593220338983051, "macro avg": {"precision": 0.7876120358514724, "recall": 0.5530840127614322, "f1-score": 0.5302653120724842, "support": 295}, "weighted avg": {"precision": 0.7732079689229366, "recall": 0.7593220338983051, "f1-score": 0.684823003013208, "support": 295}}}, "favor_or_against": {"train": {"0": {"precision": 0.9411764705882353, "recall": 0.5714285714285714, "f1-score": 0.7111111111111111, "support": 112}, "1": {"precision": 0.8814814814814815, "recall": 0.9889196675900277, "f1-score": 0.9321148825065275, "support": 361}, "accuracy": 0.8900634249471459, "macro avg": {"precision": 0.9113289760348584, "recall": 0.7801741195092995, "f1-score": 0.8216129968088193, "support": 473}, "weighted avg": {"precision": 0.8956164471896346, "recall": 0.8900634249471459, "f1-score": 0.8797841797659638, "support": 473}}, "test": {"0": {"precision": 0.5714285714285714, "recall": 0.26666666666666666, "f1-score": 0.36363636363636365, "support": 45}, "1": {"precision": 0.8316326530612245, "recall": 0.9476744186046512, "f1-score": 0.8858695652173914, "support": 172}, "accuracy": 0.8064516129032258, "macro avg": {"precision": 0.7015306122448979, "recall": 0.607170542635659, "f1-score": 0.6247529644268774, "support": 217}, "weighted avg": {"precision": 0.7776732812940844, "recall": 0.8064516129032258, "f1-score": 0.7775723575162566, "support": 217}}}}, "Legalization of Abortion": {"stance_or_none": {"train": {"0": {"precision": 0.9349593495934959, "recall": 0.7012195121951219, "f1-score": 0.8013937282229964, "support": 164}, "1": {"precision": 0.8979166666666667, "recall": 0.9817767653758542, "f1-score": 0.9379760609357998, "support": 439}, "accuracy": 0.9054726368159204, "macro avg": {"precision": 0.9164380081300814, "recall": 0.8414981387854881, "f1-score": 0.869684894579398, "support": 603}, "weighted avg": {"precision": 0.9079912935323384, "recall": 0.9054726368159204, "f1-score": 0.9008292905130804, "support": 603}}, "test": {"0": {"precision": 0.2926829268292683, "recall": 0.26666666666666666, "f1-score": 0.27906976744186046, "support": 45}, "1": {"precision": 0.8619246861924686, "recall": 0.8765957446808511, "f1-score": 0.869198312236287, "support": 235}, "accuracy": 0.7785714285714286, "macro avg": {"precision": 0.5773038065108684, "recall": 0.5716312056737589, "f1-score": 0.5741340398390737, "support": 280}, "weighted avg": {"precision": 0.7704394034376686, "recall": 0.7785714285714286, "f1-score": 0.7743562246800398, "support": 280}}}, "favor_or_against": {"train": {"0": {"precision": 0.92, "recall": 0.4380952380952381, "f1-score": 0.5935483870967742, "support": 105}, "1": {"precision": 0.8483290488431876, "recall": 0.9880239520958084, "f1-score": 0.9128630705394192, "support": 334}, "accuracy": 0.856492027334852, "macro avg": {"precision": 0.8841645244215939, "recall": 0.7130595950955232, "f1-score": 0.7532057288180967, "support": 439}, "weighted avg": {"precision": 0.8654713036756826, "recall": 0.856492027334852, "f1-score": 0.8364893991009733, "support": 439}}, "test": {"0": {"precision": 0.5769230769230769, "recall": 0.32608695652173914, "f1-score": 0.41666666666666663, "support": 46}, "1": {"precision": 0.8516746411483254, "recall": 0.9417989417989417, "f1-score": 0.8944723618090453, "support": 189}, "accuracy": 0.8212765957446808, "macro avg": {"precision": 0.7142988590357011, "recall": 0.6339429491603404, "f1-score": 0.6555695142378559, "support": 235}, "weighted avg": {"precision": 0.7978934838957236, "recall": 0.8212765957446808, "f1-score": 0.8009444385045797, "support": 235}}}}, "Climate Change is a Real Concern": {"stance_or_none": {"train": {"0": {"precision": 0.9539473684210527, "recall": 0.8630952380952381, "f1-score": 0.90625, "support": 168}, "1": {"precision": 0.9053497942386831, "recall": 0.9691629955947136, "f1-score": 0.9361702127659576, "support": 227}, "accuracy": 0.9240506329113924, "macro avg": {"precision": 0.9296485813298678, "recall": 0.9161291168449759, "f1-score": 0.9212101063829787, "support": 395}, "weighted avg": {"precision": 0.9260191422453619, "recall": 0.9240506329113924, "f1-score": 0.9234446539186641, "support": 395}}, "test": {"0": {"precision": 0.5789473684210527, "recall": 0.6285714285714286, "f1-score": 0.6027397260273972, "support": 35}, "1": {"precision": 0.9007633587786259, "recall": 0.8805970149253731, "f1-score": 0.890566037735849, "support": 134}, "accuracy": 0.8284023668639053, "macro avg": {"precision": 0.7398553635998393, "recall": 0.7545842217484009, "f1-score": 0.7466528818816232, "support": 169}, "weighted avg": {"precision": 0.8341150767519095, "recall": 0.8284023668639053, "f1-score": 0.8309570382696015, "support": 169}}}, "favor_or_against": {"train": {"0": {"precision": 0.9464285714285714, "recall": 1.0, "f1-score": 0.9724770642201834, "support": 212}, "1": {"precision": 1.0, "recall": 0.2, "f1-score": 0.33333333333333337, "support": 15}, "accuracy": 0.947136563876652, "macro avg": {"precision": 0.9732142857142857, "recall": 0.6, "f1-score": 0.6529051987767585, "support": 227}, "weighted avg": {"precision": 0.9499685336689742, "recall": 0.947136563876652, "f1-score": 0.9302428969809643, "support": 227}}, "test": {"0": {"precision": 0.917910447761194, "recall": 1.0, "f1-score": 0.9571984435797666, "support": 123}, "1": {"precision": 0.0, "recall": 0.0, "f1-score": 0.0, "support": 11}, "accuracy": 0.917910447761194, "macro avg": {"precision": 0.458955223880597, "recall": 0.5, "f1-score": 0.4785992217898833, "support": 134}, "weighted avg": {"precision": 0.8425595901091557, "recall": 0.917910447761194, "f1-score": 0.8786224519426216, "support": 134}}}}, "Feminist Movement": {"stance_or_none": {"train": {"0": {"precision": 0.9701492537313433, "recall": 0.5158730158730159, "f1-score": 0.6735751295336788, "support": 126}, "1": {"precision": 0.897822445561139, "recall": 0.9962825278810409, "f1-score": 0.9444933920704847, "support": 538}, "accuracy": 0.9051204819277109, "macro avg": {"precision": 0.9339858496462412, "recall": 0.7560777718770284, "f1-score": 0.8090342608020817, "support": 664}, "weighted avg": {"precision": 0.9115471109669308, "recall": 0.9051204819277109, "f1-score": 0.8930842036975365, "support": 664}}, "test": {"0": {"precision": 0.7, "recall": 0.3181818181818182, "f1-score": 0.4375, "support": 44}, "1": {"precision": 0.8867924528301887, "recall": 0.975103734439834, "f1-score": 0.9288537549407114, "support": 241}, "accuracy": 0.8736842105263158, "macro avg": {"precision": 0.7933962264150943, "recall": 0.6466427763108261, "f1-score": 0.6831768774703557, "support": 285}, "weighted avg": {"precision": 0.8579543197616685, "recall": 0.8736842105263158, "f1-score": 0.8529956313709174, "support": 285}}}, "favor_or_against": {"train": {"0": {"precision": 0.8482758620689655, "recall": 0.5857142857142857, "f1-score": 0.6929577464788733, "support": 210}, "1": {"precision": 0.7786259541984732, "recall": 0.9329268292682927, "f1-score": 0.8488210818307906, "support": 328}, "accuracy": 0.7973977695167286, "macro avg": {"precision": 0.8134509081337193, "recall": 0.7593205574912892, "f1-score": 0.7708894141548319, "support": 538}, "weighted avg": {"precision": 0.8058127212111189, "recall": 0.7973977695167286, "f1-score": 0.7879822334592244, "support": 538}}, "test": {"0": {"precision": 0.39285714285714285, "recall": 0.3793103448275862, "f1-score": 0.3859649122807017, "support": 58}, "1": {"precision": 0.8054054054054054, "recall": 0.8142076502732241, "f1-score": 0.8097826086956521, "support": 183}, "accuracy": 0.7095435684647303, "macro avg": {"precision": 0.5991312741312741, "recall": 0.5967589975504051, "f1-score": 0.5978737604881769, "support": 241}, "weighted avg": {"precision": 0.7061199314311347, "recall": 0.7095435684647303, "f1-score": 0.7077849888115562, "support": 241}}}}, "Atheism": {"stance_or_none": {"train": {"0": {"precision": 0.9528301886792453, "recall": 0.8632478632478633, "f1-score": 0.9058295964125562, "support": 117}, "1": {"precision": 0.9606879606879607, "recall": 0.9873737373737373, "f1-score": 0.9738480697384807, "support": 396}, "accuracy": 0.9590643274853801, "macro avg": {"precision": 0.956759074683603, "recall": 0.9253108003108004, "f1-score": 0.9398388330755185, "support": 513}, "weighted avg": {"precision": 0.9588958372473765, "recall": 0.9590643274853801, "f1-score": 0.9583350845939717, "support": 513}}, "test": {"0": {"precision": 0.5416666666666666, "recall": 0.4642857142857143, "f1-score": 0.5, "support": 28}, "1": {"precision": 0.923469387755102, "recall": 0.9427083333333334, "f1-score": 0.9329896907216495, "support": 192}, "accuracy": 0.8818181818181818, "macro avg": {"precision": 0.7325680272108843, "recall": 0.7034970238095238, "f1-score": 0.7164948453608248, "support": 220}, "weighted avg": {"precision": 0.8748763141620284, "recall": 0.8818181818181818, "f1-score": 0.8778819119025304, "support": 220}}}, "favor_or_against": {"train": {"0": {"precision": 0.9444444444444444, "recall": 0.5543478260869565, "f1-score": 0.6986301369863014, "support": 92}, "1": {"precision": 0.8801169590643275, "recall": 0.9901315789473685, "f1-score": 0.9318885448916409, "support": 304}, "accuracy": 0.8888888888888888, "macro avg": {"precision": 0.9122807017543859, "recall": 0.7722397025171626, "f1-score": 0.8152593409389711, "support": 396}, "weighted avg": {"precision": 0.8950617283950617, "recall": 0.8888888888888888, "f1-score": 0.8776971976005014, "support": 396}}, "test": {"0": {"precision": 0.45454545454545453, "recall": 0.3125, "f1-score": 0.3703703703703703, "support": 32}, "1": {"precision": 0.8705882352941177, "recall": 0.925, "f1-score": 0.896969696969697, "support": 160}, "accuracy": 0.8229166666666666, "macro avg": {"precision": 0.6625668449197861, "recall": 0.61875, "f1-score": 0.6336700336700336, "support": 192}, "weighted avg": {"precision": 0.8012477718360071, "recall": 0.8229166666666666, "f1-score": 0.8092031425364757, "support": 192}}}}}